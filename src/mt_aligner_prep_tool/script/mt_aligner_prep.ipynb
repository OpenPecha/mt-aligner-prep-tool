{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install botok\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "\n",
    "def _mkdir(path):\n",
    "    if path.is_dir():\n",
    "        return path\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    return path\n",
    "\n",
    "\n",
    "BASE_PATH = _mkdir(Path.home() / \".mt_files\")\n",
    "BO_FILES_PATH = _mkdir(BASE_PATH / \"tibetan_files\")\n",
    "EN_FILES_PATH = _mkdir(BASE_PATH / \"english_files\")\n",
    "\n",
    "\"\"\"Path to the folder where the tokenized files(both english and tibetan files) will be stored\"\"\"\n",
    "TOKENIZED_FILES_PATH = _mkdir(BASE_PATH / \"tokenized_files\")\n",
    "\n",
    "\n",
    "CHECKPOINT_FILE = BASE_PATH / \"checkpoint.json\"\n",
    "\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load the last checkpoint or create the file if it doesn't exist.\"\"\"\n",
    "    if not CHECKPOINT_FILE.exists():\n",
    "        CHECKPOINT_FILE.touch()  # Create the file if it doesn't exist\n",
    "        return []\n",
    "\n",
    "    with CHECKPOINT_FILE.open(\"r\") as file:\n",
    "        try:\n",
    "            return json.load(file)\n",
    "        except json.JSONDecodeError:\n",
    "            return {}\n",
    "\n",
    "\n",
    "def save_checkpoint(id_, stage: str, version: str = None):\n",
    "    \"\"\"\n",
    "    Save a checkpoint for a specific ID and stage.\n",
    "\n",
    "    :param id_: The ID to save the checkpoint for.\n",
    "    :param stage: The stage (e.g., 'Tokenization', 'Alignment') of the process.\n",
    "    \"\"\"\n",
    "    checkpoints = load_checkpoint()\n",
    "    if id_ not in checkpoints:\n",
    "        checkpoints[id_] = {\n",
    "            \"Tokenization\": False,\n",
    "            \"Alignment\": False,\n",
    "            \"re_alignment_versions\": [],\n",
    "        }\n",
    "\n",
    "    \"\"\"Save the checkpoint for the ID and stage.\"\"\"\n",
    "    if stage == \"re_alignment\":\n",
    "        checkpoints[id_][\"re_alignment_versions\"].append(version)\n",
    "    else:\n",
    "        checkpoints[id_][stage] = True\n",
    "\n",
    "    with CHECKPOINT_FILE.open(\"w\") as file:\n",
    "        json.dump(checkpoints, file, indent=4)\n",
    "\n",
    "\n",
    "def is_id_already_aligned(id_: str, id_checkpoints: Dict):\n",
    "    if id_ in id_checkpoints and id_checkpoints[id_][\"Alignment\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_id_already_tokenized(id_: str, id_checkpoints: Dict):\n",
    "    if id_ in id_checkpoints and id_checkpoints[id_][\"Tokenization\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_id_already_realigned(id_: str, version: Optional[str], id_checkpoints: Dict):\n",
    "    if (\n",
    "        id_ in id_checkpoints\n",
    "        and version in id_checkpoints[id_][\"re_alignment_versions\"]\n",
    "    ):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### utility.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def execution_time(custom_name=None):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            name = custom_name if custom_name else func.__name__\n",
    "            print(f\"Total time taken for {name}: {end_time - start_time} seconds.\")\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tokenizers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "import botok\n",
    "from spacy.lang.en import English\n",
    "\n",
    "bo_word_tokenizer = None\n",
    "en_nlp = English()\n",
    "en_nlp.add_pipe(\"sentencizer\")\n",
    "en_nlp.max_length = 5000000\n",
    "\n",
    "# Types\n",
    "SENT_PER_LINE_STR = str  # sentence per line string\n",
    "IS_AFFIX_PART = bool\n",
    "\n",
    "\n",
    "def get_bo_word_tokenizer():\n",
    "    global bo_word_tokenizer\n",
    "    if bo_word_tokenizer is None:\n",
    "        bo_word_tokenizer = botok.WordTokenizer()\n",
    "    return bo_word_tokenizer\n",
    "\n",
    "\n",
    "def join_sentences(sentences):\n",
    "    \"\"\"Join sentences into a text with one sentence per line.\"\"\"\n",
    "    return \"\\n\".join(sentences)\n",
    "\n",
    "\n",
    "def en_preprocess(text: str) -> str:\n",
    "    re_sub = [(r\"\\r\\n\", \" \"), (r\"\\n\", \" \"), (r\"\\s{2,}\", \" \"), (r\"\\t\", \" \")]\n",
    "    for pattern, repl in re_sub:\n",
    "        text = re.sub(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def en_sent_tokenizer(text: SENT_PER_LINE_STR) -> SENT_PER_LINE_STR:\n",
    "    \"\"\"Tokenize a text into sentences.\"\"\"\n",
    "    print(\"[INFO] Tokenizing English text...\")\n",
    "    text = en_preprocess(text)\n",
    "    doc = en_nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return join_sentences(sentences)\n",
    "\n",
    "\n",
    "def en_word_tokenizer(text: str) -> List[str]:\n",
    "    \"\"\"Tokenize a text into words.\"\"\"\n",
    "    doc = en_nlp(text)\n",
    "    words = [token.text for token in doc]\n",
    "    return words\n",
    "\n",
    "\n",
    "def bo_preprocess(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def bo_sent_tokenizer(text: str) -> SENT_PER_LINE_STR:\n",
    "    \"\"\"Tokenize a text into sentences.\"\"\"\n",
    "    print(\"[INFO] Tokenizing Tibetan text...\")\n",
    "\n",
    "    def get_token_text(token):\n",
    "        if hasattr(token, \"text_cleaned\") and token.text_cleaned:\n",
    "            return token.text_cleaned\n",
    "        else:\n",
    "            return token.text\n",
    "\n",
    "    # fmt: off\n",
    "    opening_puncts = ['༁', '༂', '༃', '༄', '༅', '༆', '༇', '༈', '༉', '༊', '༑', '༒', '༺', '༼', '༿', '࿐', '࿑', '࿓', '࿔', '࿙']  # noqa: E501\n",
    "    closing_puncts = ['།', '༎', '༏', '༐', '༔', '༴', '༻', '༽', '༾', '࿚']  # noqa: E501\n",
    "    skip_chunk_types = [botok.vars.CharMarkers.CJK.name, botok.vars.CharMarkers.LATIN.name]\n",
    "    # fmt: on\n",
    "\n",
    "    # Regex to improve the chunking of shunits, this will be replaced by a better sentence segmentation in botok\n",
    "    r_replace = [\n",
    "        (r\"༼༼[༠-༩]+[བན]༽\", r\"\"),  # delete source image numbers `ས་༼༤བ༽མེད་བ`\n",
    "        (\n",
    "            r\"([^ང])་([༔།])\",\n",
    "            r\"\\1\\2\",\n",
    "        ),  # delete spurious spaces added by botok in the cleantext values\n",
    "        (\n",
    "            r\"([།གཤ]{1,2})\\s+(།{1,2})\",\n",
    "            r\"\\1\\2 \",\n",
    "        ),  # Samdong Rinpoche style double shad. This needs to be applied on inference input\n",
    "        # (r\"\", r\"\"),\n",
    "    ]\n",
    "\n",
    "    text = bo_preprocess(text)\n",
    "    sents_words = []\n",
    "    tokenizer = get_bo_word_tokenizer()\n",
    "    tokens = tokenizer.tokenize(text, split_affixes=False)\n",
    "    for token in tokens:\n",
    "        if token.chunk_type in skip_chunk_types:\n",
    "            continue\n",
    "        token_text = get_token_text(token)\n",
    "        if any(punct in token_text for punct in opening_puncts):\n",
    "            sents_words.append(token_text.strip())\n",
    "        elif any(punct in token_text for punct in closing_puncts):\n",
    "            sents_words.append(token_text.strip())\n",
    "            sents_words.append(\"\\n\")\n",
    "        else:\n",
    "            sents_words.append(token_text)\n",
    "\n",
    "    sents_text = \"\".join(sents_words)\n",
    "\n",
    "    for fr, to in r_replace:\n",
    "        sents_text = re.sub(fr, to, sents_text)\n",
    "\n",
    "    return sents_text\n",
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emojis_to_remove = [\"1️⃣\", \"2️⃣\", \"3️⃣\"]\n",
    "    for emoji in emojis_to_remove:\n",
    "        text = text.replace(emoji, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def sent_tokenize(text, lang) -> SENT_PER_LINE_STR:\n",
    "    \"\"\"Tokenize a text into sentences.\"\"\"\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    if lang == \"en\":\n",
    "        return en_sent_tokenizer(text)\n",
    "    elif lang == \"bo\":\n",
    "        return bo_sent_tokenizer(text)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "ORG = \"MonlamAI\"\n",
    "\n",
    "\n",
    "def clone_github_repo(\n",
    "    repository,\n",
    "    destination_folder: Path,\n",
    "    organization: str = ORG,\n",
    "):\n",
    "    try:\n",
    "        if not destination_folder.exists():\n",
    "            _mkdir(destination_folder)\n",
    "            repo_url = f\"git@github.com:{organization}/{repository}.git\"\n",
    "            # Make a new folder in destination_folder and clone the repo there\n",
    "            command = [\n",
    "                \"git\",\n",
    "                \"clone\",\n",
    "                \"--no-checkout\",\n",
    "                repo_url,\n",
    "                str(destination_folder),\n",
    "            ]\n",
    "            subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise Exception(f\"Failed to clone repository {repo_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred while cloning repository {repo_url}: {e}\")\n",
    "\n",
    "\n",
    "def find_first_txt_file(folder_path: Path) -> Optional[Path]:\n",
    "    folder = Path(folder_path)\n",
    "    for file in folder.rglob(\"*.txt\"):\n",
    "        if file.is_file():\n",
    "            return file\n",
    "    raise FileNotFoundError(f\"No .txt file found in folder {folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "log_fn = \"errors.log\"\n",
    "error_id_log_fn = \"error_ids.log\"\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=str(log_fn),\n",
    "    level=logging.ERROR,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "def log_error_with_id(id_: str):\n",
    "    \"\"\"Log error message with ID to a separate file.\"\"\"\n",
    "    with open(error_id_log_fn, \"a\") as log_file:\n",
    "        log_file.write(f\"{id_}\\n\")\n",
    "\n",
    "\n",
    "def get_file_content_by_lines(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file and returns its content split into lines.\n",
    "\n",
    "    :param file_path: Path to the file to be read.\n",
    "    :return: List of lines in the file.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if file_path.exists() and file_path.is_file():\n",
    "        with file_path.open(\"r\") as file:\n",
    "            return [line.strip() for line in file if line.strip()]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No file found at {file_path}\")\n",
    "\n",
    "\n",
    "def pipeline(\n",
    "    file_path: Path, re_align: bool = False, alignment_version: Optional[str] = \"v1\"\n",
    "):\n",
    "    \"\"\"\n",
    "    file_path: a file containing ids of the repositories to be aligned\n",
    "                ,ids should be separated by new lines\n",
    "    re_align: if True, realign the ids with the specific version\n",
    "    alignment_version: version you want to name for realign\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ids = get_file_content_by_lines(file_path)\n",
    "\n",
    "    \"\"\"load progress\"\"\"\n",
    "    id_checkpoints = load_checkpoint()\n",
    "    files_tobe_aligned = []\n",
    "\n",
    "    for id_ in tqdm(ids, desc=\"Processing IDs\"):\n",
    "        try:\n",
    "            bo_id, en_id = f\"BO{id_}\", f\"EN{id_}\"\n",
    "\n",
    "            \"\"\"if id is already realigned with the specific version, skip it\"\"\"\n",
    "            if re_align and is_id_already_realigned(\n",
    "                id_, alignment_version, id_checkpoints\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            \"\"\"if id is already tokenized and aligned, skip it\"\"\"\n",
    "            if not re_align and is_id_already_aligned(id_, id_checkpoints):\n",
    "                continue\n",
    "\n",
    "            \"\"\"if id is not tokenized, tokenize it\"\"\"\n",
    "            if not is_id_already_tokenized(id_, id_checkpoints):\n",
    "                bo_file_path = BO_FILES_PATH / bo_id\n",
    "                en_file_path = EN_FILES_PATH / en_id\n",
    "\n",
    "                clone_github_repo(repository=bo_id, destination_folder=bo_file_path)\n",
    "                clone_github_repo(repository=en_id, destination_folder=en_file_path)\n",
    "\n",
    "                bo_file = find_first_txt_file(bo_file_path)\n",
    "                en_file = find_first_txt_file(en_file_path)\n",
    "                if bo_file and en_file:\n",
    "                    tokenized_bo_file_path, tokenized_en_file_path = tokenize_files(\n",
    "                        id_, bo_file, en_file\n",
    "                    )\n",
    "                    \"\"\"save the id to checkpoint file for tokenization\"\"\"\n",
    "                    save_checkpoint(id_, \"Tokenization\")\n",
    "\n",
    "            tokenized_bo_file_path = TOKENIZED_FILES_PATH / f\"tokenized_{bo_id}.txt\"\n",
    "            tokenized_en_file_path = TOKENIZED_FILES_PATH / f\"tokenized_{en_id}.txt\"\n",
    "\n",
    "            if not re_align:\n",
    "                alignment_version = None\n",
    "            files_tobe_aligned.append(\n",
    "                (\n",
    "                    id_,\n",
    "                    tokenized_bo_file_path,\n",
    "                    tokenized_en_file_path,\n",
    "                    alignment_version,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{id_}: {e}\")\n",
    "            log_error_with_id(id_)\n",
    "            continue\n",
    "    num_processes = 10\n",
    "    try:\n",
    "        with multiprocessing.Pool(num_processes) as pool:\n",
    "            pool.starmap(send_aligner_api_request, files_tobe_aligned)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Alignment Failed {id_}: {e}\")\n",
    "        log_error_with_id(id_)\n",
    "\n",
    "\n",
    "def tokenize_files(id_: str, bo_file: Path, en_file: Path):\n",
    "    bo_id, en_id = f\"BO{id_}\", f\"EN{id_}\"\n",
    "    \"\"\"Tokenize the files\"\"\"\n",
    "    tokenized_bo_text = sent_tokenize(bo_file.read_text(), lang=\"bo\")\n",
    "    tokenized_en_text = sent_tokenize(en_file.read_text(), lang=\"en\")\n",
    "\n",
    "    \"\"\"Write both tokenized texts to files in TOKENIZED_FILES_PATH\"\"\"\n",
    "    tokenized_bo_file_path = TOKENIZED_FILES_PATH / f\"tokenized_{bo_id}.txt\"\n",
    "    tokenized_en_file_path = TOKENIZED_FILES_PATH / f\"tokenized_{en_id}.txt\"\n",
    "\n",
    "    tokenized_bo_file_path.write_text(tokenized_bo_text)\n",
    "    tokenized_en_file_path.write_text(tokenized_en_text)\n",
    "\n",
    "    return tokenized_bo_file_path, tokenized_en_file_path\n",
    "\n",
    "\n",
    "@execution_time(custom_name=\"sending api request\")\n",
    "def send_aligner_api_request(\n",
    "    id_: str,\n",
    "    tokenized_bo_file_path: Path,\n",
    "    tokenized_en_file_path: Path,\n",
    "    alignment_version: Optional[str] = None,\n",
    "):\n",
    "    \n",
    "    print(f\"Sending request to aligner for {id_}\")\n",
    "\n",
    "    response = send_api_request_to_aligner(\n",
    "        id_, tokenized_bo_file_path, tokenized_en_file_path, alignment_version\n",
    "    )\n",
    "    if isinstance(response, dict) and \"error\" in response:\n",
    "        raise Exception(response[\"error\"])\n",
    "\n",
    "    print(f\"Alignment successful for {id_}\")\n",
    "    \"\"\"save the id to checkpoint file\"\"\"\n",
    "    save_checkpoint(id_, \"Alignment\")\n",
    "\n",
    "    if alignment_version:\n",
    "        \"\"\"save the id to checkpoint file for re-alignment\"\"\"\n",
    "        save_checkpoint(id_, \"re_alignment\", alignment_version)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
